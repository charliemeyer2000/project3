{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homework 3: Knowledge Distillation for AI Dermatologist\n",
        "\n",
        "## CS 4774 Machine Learning - University of Virginia\n",
        "\n",
        "In this notebook, you'll implement knowledge distillation to improve your skin disease classifier by learning from **MedSigLIP** (from Google), a powerful medical imaging model.\n",
        "\n",
        "**Key Requirements:**\n",
        "- Student model must be < **25 MB** on disk\n",
        "- Use MedSigLIP as frozen teacher model (inference only)\n",
        "- Implement temperature-scaled knowledge distillation following Hinton et al. (2015)\n",
        "\n",
        "**Recommended Starting Point:** Use ShuffleNetV2 for your student model (~5 MB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Data (Same as HW1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded with 10000 images and 10 classes\n"
          ]
        }
      ],
      "source": [
        "# Define dataset class\n",
        "class SkinDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        valid_exts = ('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.jfif')\n",
        "        for cls_name in self.classes:\n",
        "            cls_dir = os.path.join(root_dir, cls_name)\n",
        "            for fname in os.listdir(cls_dir):\n",
        "                if fname.lower().endswith(valid_exts):\n",
        "                    self.image_paths.append(os.path.join(cls_dir, fname))\n",
        "                    self.labels.append(self.class_to_idx[cls_name])\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Load data with image size\n",
        "# Training transform (Do not change)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation transform (Do not change)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = SkinDataset('training_dataset/train_dataset', transform=train_transform)\n",
        "print(f'Dataset loaded with {len(dataset)} images and {len(dataset.classes)} classes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Teacher Model (MedSigLIP from Google)\n",
        "\n",
        "**Important:** Load the pre-trained MedSigLIP model for inference only. Do NOT fine-tune it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "IMPORTANT: Before running this cell, you must:\n",
            "1. Go to https://huggingface.co/google/medsiglip-448\n",
            "2. Click 'Request Access' and wait for approval (usually instant)\n",
            "3. Get your HuggingFace token from https://huggingface.co/settings/tokens\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d52dde4875464f3e9aaded13a7da9d8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Successfully logged in to HuggingFace!\n",
            "\n",
            "Loading MedSigLIP-448 teacher model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ MedSigLIP loaded successfully!\n",
            "Student model created with 352,042 parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/charlie/all/UVA/4/F25/ml/projects/project1/.venv/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/Users/charlie/all/UVA/4/F25/ml/projects/project1/.venv/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Load MedSigLIP teacher model\n",
        "from transformers import AutoModel, AutoProcessor\n",
        "from huggingface_hub import login, HfFolder\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"IMPORTANT: Before running this cell, you must:\")\n",
        "print(\"1. Go to https://huggingface.co/google/medsiglip-448\")\n",
        "print(\"2. Click 'Request Access' and wait for approval (usually instant)\")\n",
        "print(\"3. Get your HuggingFace token from https://huggingface.co/settings/tokens\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Login to HuggingFace - this will prompt you to enter your token\n",
        "login()\n",
        "\n",
        "# Verify login\n",
        "token = HfFolder.get_token()\n",
        "if token:\n",
        "    print(\"‚úÖ Successfully logged in to HuggingFace!\")\n",
        "else:\n",
        "    print(\"‚ùå Login failed. Please try again.\")\n",
        "    raise ValueError(\"HuggingFace authentication required\")\n",
        "\n",
        "\n",
        "def load_teacher_model():\n",
        "    \"\"\"Load MedSigLIP-448 from HuggingFace.\"\"\"\n",
        "\n",
        "    print(\"\\nLoading MedSigLIP-448 teacher model...\")\n",
        "    model_name = \"google/medsiglip-448\"\n",
        "    \n",
        "    # Get token to pass explicitly\n",
        "    token = HfFolder.get_token()\n",
        "    \n",
        "    # Load model and processor with token\n",
        "    teacher_model = AutoModel.from_pretrained(\n",
        "        model_name, \n",
        "        trust_remote_code=True,\n",
        "        token=token\n",
        "    )\n",
        "    processor = AutoProcessor.from_pretrained(\n",
        "        model_name, \n",
        "        trust_remote_code=True,\n",
        "        token=token\n",
        "    )\n",
        "    \n",
        "    teacher_model = teacher_model.to(device)\n",
        "    teacher_model.eval()\n",
        "    \n",
        "    # Freeze all parameters\n",
        "    for param in teacher_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    print(\"‚úÖ MedSigLIP loaded successfully!\")\n",
        "    return teacher_model, processor\n",
        "\n",
        "# Load teacher\n",
        "teacher_model, teacher_processor = load_teacher_model()\n",
        "\n",
        "# Define student model: ShuffleNetV2 (Recommended, ~5MB)\n",
        "from torchvision.models import shufflenet_v2_x0_5\n",
        "\n",
        "def create_student_shufflenet(num_classes=10):\n",
        "    \"\"\"Create a ShuffleNetV2 student model (~5 MB).\"\"\"\n",
        "    model = shufflenet_v2_x0_5(pretrained=False)\n",
        "    # Replace final classifier\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "# Create student model\n",
        "student_model = create_student_shufflenet(num_classes=10).to(device)\n",
        "\n",
        "print(f'Student model created with {sum(p.numel() for p in student_model.parameters()):,} parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Define Distillation Loss\n",
        "\n",
        "Implement the knowledge distillation loss following Hinton et al. (2015):\n",
        "- **Hard loss**: Cross-entropy with ground truth labels\n",
        "- **Soft loss**: KL divergence between teacher and student soft predictions\n",
        "- **Temperature scaling**: Soften distributions for better knowledge transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Distillation loss initialized with temperature=4.0, alpha=0.3\n"
          ]
        }
      ],
      "source": [
        "class DistillationLoss(nn.Module):\n",
        "    def __init__(self, temperature=4.0, alpha=0.3):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "        # Initialize cross-entropy loss for hard targets\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "    \n",
        "    def forward(self, student_logits, teacher_logits, labels):\n",
        "        # Hard loss: standard cross-entropy with ground truth labels\n",
        "        hard_loss = self.ce_loss(student_logits, labels)\n",
        "        \n",
        "        # Soft loss: KL divergence between teacher and student soft predictions\n",
        "        # Temperature scaling softens the distributions for better knowledge transfer\n",
        "        student_soft = F.log_softmax(student_logits / self.temperature, dim=1)\n",
        "        teacher_soft = F.softmax(teacher_logits / self.temperature, dim=1)\n",
        "        soft_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (self.temperature ** 2)\n",
        "        \n",
        "        # Combine hard and soft losses\n",
        "        # alpha controls the balance between hard labels and soft teacher predictions\n",
        "        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n",
        "        \n",
        "        return total_loss, hard_loss, soft_loss\n",
        "\n",
        "# Create an instance of DistillationLoss\n",
        "distillation_loss = DistillationLoss(temperature=4.0, alpha=0.3)\n",
        "print(\"‚úÖ Distillation loss initialized with temperature=4.0, alpha=0.3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Train with Knowledge Distillation\n",
        "\n",
        "Implement training loop that:\n",
        "1. Gets teacher's soft predictions (with torch.no_grad())\n",
        "2. Gets student's predictions\n",
        "3. Computes distillation loss\n",
        "4. Updates only student model parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing teacher classifier head...\n",
            "‚úÖ Teacher classifier head created: 1152 -> 10 classes\n",
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [37:32<00:00,  7.99s/it]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:04<00:00,  6.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.6468 | Val Acc: 0.3470 | Val F1: 0.2195\n",
            "‚úÖ New best F1: 0.2195\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [38:48<00:00,  8.26s/it]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:04<00:00,  6.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.6217 | Val Acc: 0.4760 | Val F1: 0.3454\n",
            "‚úÖ New best F1: 0.3454\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [4:26:03<00:00, 56.61s/it]     \n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:04<00:00,  7.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.6022 | Val Acc: 0.5100 | Val F1: 0.3718\n",
            "‚úÖ New best F1: 0.3718\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [34:10<00:00,  7.27s/it]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:04<00:00,  7.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.5868 | Val Acc: 0.5300 | Val F1: 0.3959\n",
            "‚úÖ New best F1: 0.3959\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [36:01<00:00,  7.67s/it]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:04<00:00,  6.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.5761 | Val Acc: 0.5560 | Val F1: 0.4546\n",
            "‚úÖ New best F1: 0.4546\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [35:50<00:00,  7.63s/it]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:04<00:00,  7.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.5692 | Val Acc: 0.5760 | Val F1: 0.4590\n",
            "‚úÖ New best F1: 0.4590\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [37:07<00:00,  7.90s/it]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:04<00:00,  7.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.5617 | Val Acc: 0.5700 | Val F1: 0.4383\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [34:18<00:00,  7.30s/it]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:04<00:00,  7.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.5582 | Val Acc: 0.5810 | Val F1: 0.4530\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [33:42<00:00,  7.17s/it]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:04<00:00,  7.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.5524 | Val Acc: 0.5950 | Val F1: 0.4808\n",
            "‚úÖ New best F1: 0.4808\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [33:45<00:00,  7.18s/it]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:04<00:00,  7.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.5485 | Val Acc: 0.6110 | Val F1: 0.5179\n",
            "‚úÖ New best F1: 0.5179\n",
            "\n",
            "Training complete! Best F1: 0.5179\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Prepare data loaders\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
        "\n",
        "# Setup training\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=1e-3)\n",
        "criterion = distillation_loss\n",
        "\n",
        "# Initialize teacher's classifier head for generating logits\n",
        "# Note: This is a simple approach - the head generates soft targets from MedSigLIP features\n",
        "print(\"Initializing teacher classifier head...\")\n",
        "\n",
        "# MedSigLIP expects 448x448 images, so we need a resize transform for teacher\n",
        "teacher_resize = transforms.Resize((448, 448))\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Get sample batch to determine feature dimension (448x448 for MedSigLIP)\n",
        "    sample_images = torch.randn(1, 3, 448, 448).to(device)\n",
        "    teacher_features = teacher_model.vision_model(sample_images).pooler_output\n",
        "    hidden_dim = teacher_features.shape[1]\n",
        "    \n",
        "# Create classifier head (fixed random projection for soft targets)\n",
        "teacher_model.classifier_head = nn.Linear(hidden_dim, 10).to(device)\n",
        "print(f\"‚úÖ Teacher classifier head created: {hidden_dim} -> 10 classes\")\n",
        "\n",
        "# Training function\n",
        "def train_epoch(student, teacher, teacher_proc, dataloader, criterion, optimizer):\n",
        "    student.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for images, labels in tqdm(dataloader, desc='Training'):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        # Get teacher predictions (no gradients)\n",
        "        with torch.no_grad():\n",
        "            # Resize images to 448x448 for MedSigLIP (expects larger input)\n",
        "            teacher_images = F.interpolate(images, size=(448, 448), mode='bilinear', align_corners=False)\n",
        "            # Get MedSigLIP vision embeddings and project to class logits\n",
        "            teacher_features = teacher.vision_model(teacher_images).pooler_output\n",
        "            teacher_logits = teacher.classifier_head(teacher_features)\n",
        "        \n",
        "        # Get student predictions\n",
        "        student_logits = student(images)\n",
        "        \n",
        "        # Compute distillation loss\n",
        "        loss, hard_loss, soft_loss = criterion(student_logits, teacher_logits, labels)\n",
        "        \n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Validation function\n",
        "def validate(student, dataloader):\n",
        "    student.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc='Validation'):\n",
        "            images = images.to(device)\n",
        "            outputs = student(images)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            \n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "    \n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return accuracy, f1\n",
        "\n",
        "# Training loop\n",
        "NUM_EPOCHS = 10\n",
        "best_f1 = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f'\\nEpoch {epoch+1}/{NUM_EPOCHS}')\n",
        "    \n",
        "    # Train\n",
        "    train_loss = train_epoch(student_model, teacher_model, teacher_processor, \n",
        "                             train_loader, criterion, optimizer)\n",
        "    \n",
        "    # Validate\n",
        "    val_acc, val_f1 = validate(student_model, val_loader)\n",
        "    \n",
        "    print(f'Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}')\n",
        "    \n",
        "    if val_f1 > best_f1:\n",
        "        best_f1 = val_f1\n",
        "        print(f'‚úÖ New best F1: {best_f1:.4f}')\n",
        "\n",
        "print(f'\\nTraining complete! Best F1: {best_f1:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Save and Submit\n",
        "\n",
        "Save your student model (< 25 MB) and submit to the HW3 leaderboard.\n",
        "\n",
        "**Important:** Only submit the student model, NOT the teacher!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model saved: student_model_hw3.pt\n",
            "üì¶ Model size: 1.53 MB\n",
            "‚úÖ Model size is within the 25 MB limit\n",
            "\n",
            "üéØ View the HW3 leaderboard at: http://hadi.cs.virginia.edu:8000/leaderboard3\n"
          ]
        }
      ],
      "source": [
        "# Save student model\n",
        "student_model.eval()\n",
        "student_model.cpu()\n",
        "scripted_model = torch.jit.script(student_model)\n",
        "scripted_model.save('student_model_hw3.pt')\n",
        "\n",
        "# Check model size\n",
        "import os\n",
        "size_mb = os.path.getsize('student_model_hw3.pt') / (1024 * 1024)\n",
        "print(f'‚úÖ Model saved: student_model_hw3.pt')\n",
        "print(f'üì¶ Model size: {size_mb:.2f} MB')\n",
        "\n",
        "if size_mb >= 25.0:\n",
        "    print('‚ùå WARNING: Model exceeds 25 MB limit!')\n",
        "else:\n",
        "    print('‚úÖ Model size is within the 25 MB limit')\n",
        "\n",
        "# Submit to HW3 leaderboard\n",
        "def submit_model(token, model_path, server_url='http://hadi.cs.virginia.edu:8000'):\n",
        "    \"\"\"Submit model to the HW3 leaderboard.\"\"\"\n",
        "    with open(model_path, 'rb') as f:\n",
        "        files = {'file': f}\n",
        "        data = {'token': token}\n",
        "        response = requests.post(f'{server_url}/submit', data=data, files=files)\n",
        "        resp_json = response.json()\n",
        "        if 'message' in resp_json:\n",
        "            print(f\"‚úÖ {resp_json['message']}\")\n",
        "        else:\n",
        "            print(f\"‚ùå {resp_json.get('error', 'Unknown error')}\")\n",
        "\n",
        "# Check submission status\n",
        "def check_status(token, server_url='http://hadi.cs.virginia.edu:8000'):\n",
        "    \"\"\"Check your submission status.\"\"\"\n",
        "    url = f'{server_url}/submission-status/{token}'\n",
        "    response = requests.get(url)\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        attempts = response.json()\n",
        "        for a in attempts:\n",
        "            score = f\"{a['score']:.4f}\" if isinstance(a['score'], (float, int)) else \"Pending\"\n",
        "            size = f\"{a['model_size']:.2f}\" if isinstance(a['model_size'], (float, int)) else \"N/A\"\n",
        "            print(f\"Attempt {a['attempt']}: Score={score}, Size={size} MB, Status={a['status']}\")\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "\n",
        "# Use your token from registration\n",
        "my_token = 'your_token_here'\n",
        "\n",
        "# Uncomment to submit:\n",
        "# submit_model(my_token, 'student_model_hw3.pt')\n",
        "# check_status(my_token)\n",
        "\n",
        "print('\\nüéØ View the HW3 leaderboard at: http://hadi.cs.virginia.edu:8000/leaderboard3')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
